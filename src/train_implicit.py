import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import os
import numpy as np
import time

# ê²½ë¡œ ë¬¸ì œ ë°©ì§€ ë° ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from dataset import NICORobotDataset, get_normalization_stats, TRAIN_DIR, TEST_DIR
    from energy_model import EnergyModel
except ImportError:
    import sys
    sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
    from dataset import NICORobotDataset, get_normalization_stats, TRAIN_DIR, TEST_DIR
    from models.energy_model import EnergyModel

# ---------------------------------------------------------
# 1. Hard Negative Sampling
# ---------------------------------------------------------
def generate_negatives(pos_actions, num_negatives, action_dim, device):
    batch_size = pos_actions.shape[0]
    
    # [1] ì •ë‹µ ë³µì‚¬
    neg_actions = pos_actions.unsqueeze(1).repeat(1, num_negatives, 1)
    
    # [2] Hard Negative (50%): ì •ë‹µì— ë…¸ì´ì¦ˆ ì„ê¸°
    noise = torch.randn_like(neg_actions) * 0.3
    neg_actions = neg_actions + noise
    
    # [3] Random Negative (50%): ì™„ì „ ë¬´ì‘ìœ„
    num_random = num_negatives // 2
    random_noise = torch.rand(batch_size, num_random, action_dim, device=device) * 2 - 1
    
    neg_actions[:, :num_random, :] = random_noise
    
    # [4] í´ë¦¬í•‘ (-1 ~ 1)
    neg_actions = neg_actions.view(-1, action_dim)
    return torch.clamp(neg_actions, -1.0, 1.0)

# ---------------------------------------------------------
# 2. Implicit BC í•™ìŠµ ë£¨í”„ (Main)
# ---------------------------------------------------------
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[Info] Using device: {device}")
    
    action_dim = 14
    num_negatives = 64
    batch_size = 32
    
    # [ì¤‘ìš”] ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ì—í­ 100ìœ¼ë¡œ ì„¤ì •
    epochs = 100      
    lr = 1e-4
    
    checkpoint_path = "latest_checkpoint.pth"
    best_model_path = "best_implicit_model.pth"

    # 1. ë°ì´í„°ì…‹ ì¤€ë¹„
    csv_path = os.path.join(TRAIN_DIR, 'samples.csv')
    if not os.path.exists(csv_path):
        print(f"[Error] ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        return

    stats = get_normalization_stats(csv_path)
    # Augmentationì´ ì ìš©ëœ ë°ì´í„°ì…‹ ì‚¬ìš© (is_train=True)
    train_ds = NICORobotDataset(TRAIN_DIR, stats, is_train=True)
    test_ds = NICORobotDataset(TEST_DIR, stats, is_train=False)

    # CPUì—ì„œëŠ” num_workersë¥¼ 0 ë˜ëŠ” 2 ì •ë„ë¡œ ì‘ê²Œ ì„¤ì •
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)
    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)

    # 2. ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
    model = EnergyModel(action_dim=action_dim, stats=stats, device=device).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()
    
    writer = SummaryWriter('runs/implicit_experiment_best_tracking')

    # ---------------------------------------------------------
    # [í•µì‹¬] Resume Logic (ì¤‘ë‹¨ëœ ê³³ë¶€í„° ì´ì–´í•˜ê¸°)
    # ---------------------------------------------------------
    start_epoch = 0
    global_step = 0
    best_val_loss = float('inf')
    
    if os.path.exists(checkpoint_path):
        print(f"\n[Info] Found checkpoint: {checkpoint_path}")
        try:
            checkpoint = torch.load(checkpoint_path, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            # ì €ì¥ëœ ì—í­ ë‹¤ìŒë¶€í„° ì‹œì‘
            start_epoch = checkpoint['epoch'] + 1
            global_step = checkpoint['global_step']
            
            if 'best_val_loss' in checkpoint:
                best_val_loss = checkpoint['best_val_loss']
                
            print(f" -> ğŸ”„ Resuming from Epoch {start_epoch+1}")
            print(f" -> Current Best Val Loss: {best_val_loss:.4f}")
        except Exception as e:
            print(f"[Warning] Failed to load checkpoint: {e}")
            print(" -> Starting from scratch.")
    else:
        print("\n[Info] Starting FRESH training!")

    print(f"\n[Info] Start Training (Epochs: {start_epoch+1} ~ {epochs})...")
    
    total_batches = len(train_loader)

    # 3. í•™ìŠµ ë£¨í”„ ì‹œì‘
    for epoch in range(start_epoch, epochs):
        model.train()
        train_loss_sum = 0.0
        start_time = time.time()
        
        for i, batch in enumerate(train_loader):
            images = batch['image'].to(device)
            pos_actions = batch['action'].to(device)
            
            # (A) Positive Energy
            pos_energy = model(images, pos_actions)
            
            # (B) Negative Energy
            neg_actions = generate_negatives(pos_actions, num_negatives, action_dim, device)
            
            images_expanded = images.unsqueeze(1).repeat(1, num_negatives, 1, 1, 1).view(-1, 6, 64, 64)
            neg_energy = model(images_expanded, neg_actions).view(batch_size, num_negatives)
            
            # (C) Loss Calculation (InfoNCE)
            logits = torch.cat([-pos_energy, -neg_energy], dim=1)
            labels = torch.zeros(batch_size, dtype=torch.long, device=device)
            
            loss = criterion(logits, labels)
            
            # (D) Backpropagation
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_loss_sum += loss.item()
            writer.add_scalar('Loss/train_step', loss.item(), global_step)
            global_step += 1
            
            # ë¡œê·¸ ì¶œë ¥ (10 ë°°ì¹˜ë§ˆë‹¤)
            if (i + 1) % 10 == 0:
                print(f"\rEpoch [{epoch+1}/{epochs}] Step [{i+1}/{total_batches}] Loss: {loss.item():.4f}", end="")

        avg_train_loss = train_loss_sum / len(train_loader)
        writer.add_scalar('Loss/train_epoch', avg_train_loss, epoch)
        
        epoch_duration = time.time() - start_time
        print(f"\nTime per Epoch: {epoch_duration:.2f} sec")

        # --- Validation ---
        print("Running Validation...", end="")
        model.eval()
        val_loss_sum = 0.0
        with torch.no_grad():
            for batch in test_loader:
                images = batch['image'].to(device)
                pos_actions = batch['action'].to(device)
                
                pos_energy = model(images, pos_actions)
                
                neg_actions = generate_negatives(pos_actions, num_negatives, action_dim, device)
                
                images_expanded = images.unsqueeze(1).repeat(1, num_negatives, 1, 1, 1).view(-1, 6, 64, 64)
                neg_energy = model(images_expanded, neg_actions).view(batch_size, num_negatives)
                
                logits = torch.cat([-pos_energy, -neg_energy], dim=1)
                labels = torch.zeros(batch_size, dtype=torch.long, device=device)
                
                loss = criterion(logits, labels)
                val_loss_sum += loss.item()

        avg_val_loss = val_loss_sum / len(test_loader)
        writer.add_scalar('Loss/validation', avg_val_loss, epoch)
        
        print(f"\rEpoch [{epoch+1}/{epochs}] | Train: {avg_train_loss:.4f} | Val: {avg_val_loss:.4f}", end="")

        # Best Model ì €ì¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), best_model_path)
            print(f" -> ğŸ† New Best Model! ({best_val_loss:.4f})")
        else:
            print("") 

        # ---------------------------------------------------------
        # [í•µì‹¬] ë§¤ ì—í­ë§ˆë‹¤ ìƒíƒœ ì €ì¥ (ëŠê²¨ë„ ì—¬ê¸°ì„œë¶€í„° ì‹œì‘)
        # ---------------------------------------------------------
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'global_step': global_step,
            'best_val_loss': best_val_loss,
            'loss': avg_train_loss,
        }, checkpoint_path)

    # ìµœì¢… ì €ì¥
    torch.save(model.state_dict(), "implicit_bc_final.pth")
    writer.close()
    print("\n[Success] Training Complete!")

if __name__ == "__main__":
    main()